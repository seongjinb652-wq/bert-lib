"""
File: hub_upload.py
Author: ì„±ì§„
Date: 2026-01-18

Description:
    Hugging Face Transformersì™€ huggingface_hubë¥¼ í™œìš©í•˜ì—¬
    í•œêµ­ì–´ BERT/RoBERTa ê¸°ë°˜ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  [MASK] í† í° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œ ë’¤,
    Hugging Face Hubì— ì—…ë¡œë“œí•˜ëŠ” ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤.

Features:
    - klue/roberta-small ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    - [MASK] í† í° ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸ (top-k í›„ë³´ ì¶œë ¥)
    - ëª¨ë¸ ì„¤ì • ì •ë³´ ì¶œë ¥ (íˆë“  í¬ê¸°, ì–´í…ì…˜ í—¤ë“œ ìˆ˜, ë ˆì´ì–´ ìˆ˜, ì–´íœ˜ í¬ê¸°)
    - Hugging Face Hub ë¡œê·¸ì¸ ì•ˆë‚´
    - Trainer ê¸°ë°˜ ìë™ ì—…ë¡œë“œ ì„¤ì • (push_to_hub=True)
    - push_to_hub() ë©”ì†Œë“œë¡œ ìˆ˜ë™ ì—…ë¡œë“œ ì˜ˆì‹œ ì œê³µ

Dependencies:
    - transformers
    - torch
    - huggingface_hub

Usage:
    1. Hugging Face Hub í† í° ë°œê¸‰ ë° notebook_login() ì‹¤í–‰
    2. Trainerë¡œ í•™ìŠµ ì‹œ push_to_hub=True ì˜µì…˜ì„ í†µí•´ ìë™ ì—…ë¡œë“œ
    3. ë˜ëŠ” model.push_to_hub(), tokenizer.push_to_hub()ë¡œ ìˆ˜ë™ ì—…ë¡œë“œ ê°€ëŠ¥

Note:
    - 'YOUR_HF_USERNAME'ì„ ìì‹ ì˜ Hugging Face ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨
    - repo_idëŠ” "ì‚¬ìš©ìëª…/ëª¨ë¸ì´ë¦„" í˜•ì‹ìœ¼ë¡œ ì§€ì •
"""

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# !pip install transformers torch -q  import torch
from transformers import AutoTokenizer, AutoModelForMaskedLM

# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
checkpoint = "klue/roberta-small"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForMaskedLM.from_pretrained(checkpoint)

# 2. ì…ë ¥ ë¬¸ì¥ ì¤€ë¹„
test_sentence = "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” [MASK]ì´ë‹¤."
print(f"ğŸ“ ì…ë ¥ ë¬¸ì¥: {test_sentence}")

# 3. í† í°í™” (PyTorch í…ì„œë¡œ ë³€í™˜)
inputs = tokenizer(test_sentence, return_tensors="pt")

# 4. [MASK] í† í°ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤) ì°¾ê¸°
# ì…ë ¥ëœ ë¬¸ì¥ ë‚´ì—ì„œ mask_token_idë¥¼ ê°€ì§„ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

# 5. ëª¨ë¸ ì¶”ë¡  (Forward Pass)
# ëª¨ë¸ì— ì…ë ¥ì„ ë„£ê³  ì˜ˆì¸¡ê°’ì„ ë°›ìŠµë‹ˆë‹¤.
with torch.no_grad():
    outputs = model(**inputs)

    # ì—¬ê¸°ì„œ íŠœí”Œë¡œ ë‚˜ì˜¤ë“  ë”•ì…”ë„ˆë¦¬ë¡œ ë‚˜ì˜¤ë“  ìƒê´€ì—†ì´ ì²« ë²ˆì§¸ ìš”ì†Œ(Logits)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    if isinstance(outputs, tuple):
        logits = outputs[0]
    else:
        logits = outputs.logits

# 6. ê²°ê³¼ í™•ì¸
# [MASK] ìœ„ì¹˜ì˜ ë¡œì§“(ì ìˆ˜)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
mask_token_logits = logits[0, mask_token_index, :]

# ìƒìœ„ 5ê°œ í›„ë³´ ë½‘ê¸° (topk)
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

print("\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:")
for i, token_id in enumerate(top_5_tokens, 1):
    prediction = tokenizer.decode([token_id])
    print(f"  {i}. {prediction}")    # 3. Hub ëª¨ë¸ ì •ë³´ í™•ì¸í•˜ê¸°
print("\n" + "=" * 60)
print("ğŸ“‹ ëª¨ë¸ ì„¤ì • ì •ë³´")
print("=" * 60)

print(f"\nëª¨ë¸ ì´ë¦„: {checkpoint}")
print(f"íˆë“  í¬ê¸°: {model.config.hidden_size}")
print(f"ì–´í…ì…˜ í—¤ë“œ ìˆ˜: {model.config.num_attention_heads}")
print(f"ë ˆì´ì–´ ìˆ˜: {model.config.num_hidden_layers}")
print(f"ì–´íœ˜ í¬ê¸°: {model.config.vocab_size:,}") from huggingface_hub import notebook_login

# Colab/Jupyter í™˜ê²½ì—ì„œ ë¡œê·¸ì¸
# ì‹¤í–‰í•˜ë©´ í† í° ì…ë ¥ ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤
# https://huggingface.co/settings/tokens ì—ì„œ í† í°ì„ ë°œê¸‰ë°›ìœ¼ì„¸ìš”

print("=" * 60)
print("ğŸ” Hugging Face Hub ë¡œê·¸ì¸")
print("=" * 60)
print("\nì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ í† í° ì…ë ¥ ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
print("Hub ì„¤ì • í˜ì´ì§€ì—ì„œ 'write' ê¶Œí•œì´ ìˆëŠ” í† í°ì„ ë°œê¸‰ë°›ì•„ ì…ë ¥í•˜ì„¸ìš”.")
print("\ní† í° ë°œê¸‰: https://huggingface.co/settings/tokens")    # ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰í•˜ì„¸ìš”
# notebook_login()    print("=" * 60)
print("ğŸš€ Trainerë¥¼ ì´ìš©í•œ Hub ì—…ë¡œë“œ ì„¤ì •")
print("=" * 60)

from transformers import TrainingArguments, Trainer

# Hub ì—…ë¡œë“œë¥¼ ìœ„í•œ TrainingArguments ì˜ˆì‹œ
# âš ï¸ 'YOUR_HF_USERNAME'ì„ ìì‹ ì˜ í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!

hub_training_args = TrainingArguments(
    output_dir="./nsmc-finetuned-bert",
    eval_strategy="epoch",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    learning_rate=2e-5,
    # Hub ì—…ë¡œë“œ ê´€ë ¨ ì„¤ì •
    push_to_hub=True,  # í›ˆë ¨ ì™„ë£Œ í›„ ìë™ ì—…ë¡œë“œ
    hub_model_id="YOUR_HF_USERNAME/nsmc-finetuned-bert",  # "ì‚¬ìš©ìëª…/ëª¨ë¸ì´ë¦„"
)

print("\nğŸ“‹ Hub ì—…ë¡œë“œ ì„¤ì •:")
print(f"  - push_to_hub: {hub_training_args.push_to_hub}")
print(f"  - hub_model_id: {hub_training_args.hub_model_id}")
print(f"  - output_dir: {hub_training_args.output_dir}")

print("\nğŸ’¡ Trainerë¡œ í›ˆë ¨í•˜ë©´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë©ë‹ˆë‹¤!")
print("   trainer = Trainer(model=model, args=hub_training_args, ...)")
print("   trainer.train()  # í›ˆë ¨ ì™„ë£Œ í›„ ìë™ ì—…ë¡œë“œ")    print("=" * 60)
print("ğŸš€ push_to_hub() ë©”ì†Œë“œë¡œ ì§ì ‘ ì—…ë¡œë“œ")
print("=" * 60)

# âš ï¸ 'YOUR_HF_USERNAME'ì„ ìì‹ ì˜ í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!
repo_id = "YOUR_HF_USERNAME/nsmc-finetuned-bert-manual"

print(f"\nğŸ“¦ ì €ì¥ì†Œ ID: {repo_id}")
print("\nğŸ”§ ì—…ë¡œë“œ ì½”ë“œ ì˜ˆì‹œ:")
print("""
# ëª¨ë¸ ë¡œì»¬ ì €ì¥
model.save_pretrained("./my-local-model")
tokenizer.save_pretrained("./my-local-model")

# Hubì— ì—…ë¡œë“œ
model.push_to_hub(repo_id)       # ëª¨ë¸ ì—…ë¡œë“œ
tokenizer.push_to_hub(repo_id)   # í† í¬ë‚˜ì´ì € ì—…ë¡œë“œ
""")

print("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ê°ê° push_to_hub()ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
print("ğŸ’¡ ê°™ì€ repo_idë¥¼ ì‚¬ìš©í•˜ë©´ ê°™ì€ ì €ì¥ì†Œì— í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.")

# ì‹¤ì œ ì—…ë¡œë“œ (ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰)
# model.push_to_hub(repo_id)
# tokenizer.push_to_hub(repo_id)  
