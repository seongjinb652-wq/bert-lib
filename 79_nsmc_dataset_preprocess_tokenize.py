"""
File: nsmc_dataset_preprocess_tokenize.py
Author: ì„±ì§„
Date: 2026-01-18

Description:
    NSMC(Naver Sentiment Movie Corpus) ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³ ,
    ì „ì²˜ë¦¬ ë° í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ Hugging Face Trainerì— ì…ë ¥í•  ìˆ˜ ìˆëŠ”
    ìµœì¢… PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

Features:
    - NSMC ë°ì´í„°ì…‹ ë¡œë“œ (train/test CSV íŒŒì¼)
    - ë°ì´í„° í†µê³„ ë° ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    - ê²°ì¸¡ì¹˜(None) ì œê±°
    - AutoTokenizerë¥¼ ì´ìš©í•œ ë¬¸ì¥ í† í°í™” (padding, truncation, max_length=128)
    - í† í°í™” ê²°ê³¼ í™•ì¸ (input_ids, labels)
    - ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° ë ˆì´ë¸” ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
    - PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

Dependencies:
    - datasets
    - transformers
    - torch
    - collections (Counter)

Usage:
    $ python nsmc_dataset_preprocess_tokenize.py
    â†’ ë°ì´í„°ì…‹ ë¡œë“œ, ì „ì²˜ë¦¬, í† í°í™” ê²°ê³¼ë¥¼ í™•ì¸ ê°€ëŠ¥

Note:
    - checkpoint ë³€ìˆ˜ì— ì‚¬ìš©í•  ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì§€ì • í•„ìš” (ì˜ˆ: "klue/bert-base")
    - ìµœì¢… ê²°ê³¼(tokenized_datasets)ëŠ” Trainer í•™ìŠµ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë¨
"""
from datasets import load_dataset

# NSMC ì›ë³¸ ë°ì´í„°(GitHub) ì£¼ì†Œ ì„¤ì •
data_files = {
    "train": "https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt",
    "test": "https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
}

# CSV ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ (êµ¬ë¶„ìëŠ” íƒ­ '\t')
raw_datasets = load_dataset("csv", data_files=data_files, delimiter="\t")

print("ğŸ“¦ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(raw_datasets)

print("\nğŸ“Š í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ:")
# ìƒ˜í”Œ ì¶œë ¥ (ì»¬ëŸ¼ëª…: id, document, label)
print(raw_datasets["train"][0])
print(raw_datasets["train"][1])  # ë°ì´í„° í†µê³„ í™•ì¸
print("ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„:")
print(f"   í•™ìŠµ ë°ì´í„°: {len(raw_datasets['train']):,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(raw_datasets['test']):,}ê°œ")

# ë ˆì´ë¸” ë¶„í¬ í™•ì¸
from collections import Counter

train_labels = raw_datasets["train"]["label"]
label_counts = Counter(train_labels)
print(f"\nğŸ·ï¸ ë ˆì´ë¸” ë¶„í¬:")
print(f"   ë¶€ì •(0): {label_counts[0]:,}ê°œ ({label_counts[0] / len(train_labels):.1%})")
print(f"   ê¸ì •(1): {label_counts[1]:,}ê°œ ({label_counts[1] / len(train_labels):.1%})")  # 1. ê²°ì¸¡ì¹˜(None)ê°€ ìˆëŠ” í–‰ ì œê±°
print(f"ì „ì²˜ë¦¬ ì „ ë°ì´í„° ê°œìˆ˜: {len(raw_datasets['train'])}")

# document ì»¬ëŸ¼ì´ Noneì´ ì•„ë‹Œ ê²ƒë§Œ ë‚¨ê¹€
raw_datasets = raw_datasets.filter(lambda x: x["document"] is not None)

print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° ê°œìˆ˜: {len(raw_datasets['train'])}")  from transformers import AutoTokenizer

checkpoint = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


# í† í°í™” í•¨ìˆ˜ ì •ì˜
def tokenize_function(examples):
    """
    ë°ì´í„°ì…‹ì˜ 'document' ì»¬ëŸ¼ì„ í† í°í™”
    """
    return tokenizer(
        examples["document"],
        padding="max_length",  # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©
        truncation=True,  # ê¸¸ë©´ ìë¥´ê¸°
        max_length=128,  # ìµœëŒ€ 128 í† í°
    )


# ì „ì²´ ë°ì´í„°ì…‹ì— í† í°í™” ì ìš©
# batched=True: ì—¬ëŸ¬ ìƒ˜í”Œì„ í•œ ë²ˆì— ì²˜ë¦¬ (ë¹ ë¦„!)
print("ğŸ”„ í† í°í™” ì§„í–‰ ì¤‘...")
tokenized_datasets = raw_datasets.map(
    tokenize_function, batched=True, desc="Tokenizing"
)

print("\nâœ… í† í°í™” ì™„ë£Œ!")
print(f"   ì»¬ëŸ¼: {tokenized_datasets['train'].column_names}")    # í† í°í™” ê²°ê³¼ í™•ì¸
sample = tokenized_datasets["train"][0]
print("ğŸ“ í† í°í™” ê²°ê³¼ ì˜ˆì‹œ:")
print(f"   ì›ë³¸: {raw_datasets['train'][0]['document'][:50]}...")
print(f"   input_ids ê¸¸ì´: {len(sample['input_ids'])}")
print(f"   ë ˆì´ë¸”: {sample['label']}")  # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
tokenized_datasets = tokenized_datasets.remove_columns(["id", "document"])

# ë ˆì´ë¸” ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ (Trainerê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ì„¤ì •
tokenized_datasets.set_format("torch")

print("ğŸ“¦ ìµœì¢… ë°ì´í„°ì…‹ í˜•íƒœ:")
print(f"   ì»¬ëŸ¼: {tokenized_datasets['train'].column_names}")
print(f"   íƒ€ì…: {type(tokenized_datasets['train'][0]['input_ids'])}")   
