"""
File: bert_sentiment_nsmc_finetune.py .py
Author: ì„±ì§„
Date: 2026-01-18

Description:
    Hugging Face Transformersì˜ Trainerë¥¼ í™œìš©í•˜ì—¬
    NSMC(Naver Sentiment Movie Corpus) ë°ì´í„°ì…‹ìœ¼ë¡œ
    ê°ì„± ë¶„ì„ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

Features:
    - TrainingArguments ì„¤ì • (ì¶œë ¥ ë””ë ‰í† ë¦¬, í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ì—í­ ë“±)
    - compute_metrics í•¨ìˆ˜ ì •ì˜ (Accuracy, F1 ì ìˆ˜)
    - AutoModelForSequenceClassification ë¡œë“œ (ê¸ì •/ë¶€ì • 2ê°œ ë ˆì´ë¸”)
    - Trainer ê°ì²´ ìƒì„± ë° í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ ì—°ê²°
    - í•™ìŠµ ì „ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ í‰ê°€
    - íŒŒì¸íŠœë‹ í•™ìŠµ ì‹¤í–‰ ë° í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€
    - ì •í™•ë„ í–¥ìƒ ê³„ì‚° ë° ì¶œë ¥

Dependencies:
    - transformers
    - datasets
    - evaluate
    - numpy
    - torch

Usage:
    $ python bert_sentiment_nsmc_finetune.py 
    â†’ í•™ìŠµ ì „í›„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ë¥¼ í™•ì¸ ê°€ëŠ¥

Note:
    - checkpoint ë³€ìˆ˜ì— ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì´ë¦„ì„ ì§€ì •í•´ì•¼ í•¨ (ì˜ˆ: "klue/bert-base")
    - tokenized_datasetsëŠ” ì‚¬ì „ì— í† í°í™”ëœ NSMC ë°ì´í„°ì…‹ì´ì–´ì•¼ í•¨
"""
from transformers import TrainingArguments

training_args = TrainingArguments(
    # ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir="./nsmc-finetuned-bert",
    # ğŸ“Š í•™ìŠµ ì„¤ì •
    num_train_epochs=1,  # ì „ì²´ ë°ì´í„°ë¥¼ 1ë²ˆ í•™ìŠµ (ì‹œê°„ ì ˆì•½)
    per_device_train_batch_size=32,  # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
    per_device_eval_batch_size=64,  # í‰ê°€ ë°°ì¹˜ í¬ê¸°
    # ğŸ“ˆ í‰ê°€ ì„¤ì •
    eval_strategy="epoch",  # ë§¤ ì—í­ë§ˆë‹¤ í‰ê°€
    save_strategy="epoch",  # ë§¤ ì—í­ë§ˆë‹¤ ì €ì¥
    # âš™ï¸ ìµœì í™” ì„¤ì •
    learning_rate=2e-5,  # í•™ìŠµë¥  (BERT ê¶Œì¥ê°’)
    weight_decay=0.01,  # ê°€ì¤‘ì¹˜ ê°ì‡ 
    # ğŸ“ ë¡œê¹…
    logging_steps=500,  # 500ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    # ğŸ”§ ê¸°íƒ€
    load_best_model_at_end=True,  # í•™ìŠµ í›„ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ
    metric_for_best_model="accuracy",  # ìµœê³  ëª¨ë¸ ê¸°ì¤€
    # âš¡ ì„±ëŠ¥ ìµœì í™” (GPU ì‚¬ìš© ì‹œ)
    # fp16=True,                     # í˜¼í•© ì •ë°€ë„ í•™ìŠµ
)

print("âœ… TrainingArguments ì„¤ì • ì™„ë£Œ!")
print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {training_args.output_dir}")
print(f"   í•™ìŠµ ì—í­: {training_args.num_train_epochs}")
print(f"   ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
print(f"   í•™ìŠµë¥ : {training_args.learning_rate}")      import numpy as np
import evaluate

# í‰ê°€ì§€í‘œ ë¡œë“œ
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")


def compute_metrics(eval_pred):
    """
    í‰ê°€ì§€í‘œ ê³„ì‚° í•¨ìˆ˜

    Args:
        eval_pred: (logits, labels) íŠœí”Œ

    Returns:
        dict: {"accuracy": ..., "f1": ...}
    """
    logits, labels = eval_pred

    # logitsì—ì„œ ì˜ˆì¸¡ í´ë˜ìŠ¤ ì¶”ì¶œ
    predictions = np.argmax(logits, axis=-1)

    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)

    # F1 ì ìˆ˜ ê³„ì‚°
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="binary")

    return {"accuracy": accuracy["accuracy"], "f1": f1["f1"]}


print("âœ… í‰ê°€ì§€í‘œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!")   from transformers import (
    AutoModelForSequenceClassification,
    Trainer,
    DataCollatorWithPadding,
)

# ëª¨ë¸ ë¡œë“œ (2ê°œ ë ˆì´ë¸”: ê¸ì •/ë¶€ì •)
model = AutoModelForSequenceClassification.from_pretrained(
    checkpoint,
    num_labels=2,
    id2label={0: "ë¶€ì •", 1: "ê¸ì •"},
    label2id={"ë¶€ì •": 0, "ê¸ì •": 1},
)

print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
print(f"   ë ˆì´ë¸” ë§¤í•‘: {model.config.id2label}")     # ë°ì´í„° ì½œë ˆì´í„° (ë™ì  íŒ¨ë”©)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("âœ… Trainer ì„¤ì • ì™„ë£Œ!")     # í•™ìŠµ ì „ ë² ì´ìŠ¤ë¼ì¸ í‰ê°€
print("ğŸ“Š í•™ìŠµ ì „ ëª¨ë¸ ì„±ëŠ¥ (ë² ì´ìŠ¤ë¼ì¸):")
baseline_results = trainer.evaluate()
print(f"   Accuracy: {baseline_results['eval_accuracy']:.4f}")
print(f"   F1 Score: {baseline_results['eval_f1']:.4f}")    # ğŸš€ í•™ìŠµ ì‹œì‘!
print("\nğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘!")
print("=" * 50)

train_result = trainer.train()

print("=" * 50)
print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")     # í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€
print("\nğŸ“Š í•™ìŠµ í›„ ëª¨ë¸ ì„±ëŠ¥:")
final_results = trainer.evaluate()
print(f"   Accuracy: {final_results['eval_accuracy']:.4f}")
print(f"   F1 Score: {final_results['eval_f1']:.4f}")

# ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
acc_improvement = final_results["eval_accuracy"] - baseline_results["eval_accuracy"]
print(f"\nğŸ“ˆ ì •í™•ë„ í–¥ìƒ: {acc_improvement:+.4f}")  
