"""
File: bert_finetune_eval.py
Author: μ„±μ§„
Date: 2026-01-18

Description:
    Hugging Face Transformersμ pipelineμ„ ν™μ©ν•μ—¬
    νμΈνλ‹λ ν…μ¤νΈ λ¶„λ¥ λ¨λΈμ„ ν‰κ°€ν•λ” μμ  μ½”λ“μ…λ‹λ‹¤.

Features:
    - νμΈνλ‹λ λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ €λ¥Ό pipelineμΌλ΅ λ΅λ“
    - ν…μ¤νΈ μƒν”μ— λ€ν•΄ μμΈ΅ μν–‰
    - μ •λ‹µ λΌλ²¨κ³Ό λΉ„κµν•μ—¬ μ •ν™•λ„ κ³„μ‚°
    - κ²°κ³Όλ¥Ό μ§κ΄€μ μΌλ΅ μ¶λ ¥ (β…/β ν‘μ‹ λ° μ‹ λΆ°λ„)

Dependencies:
    - transformers (pipeline)
    - νμΈνλ‹λ λ¨λΈ λ° ν† ν¬λ‚μ΄μ €

Usage:
    1. νμΈνλ‹λ λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ €λ¥Ό μ¤€λΉ„
    2. λ³Έ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•λ©΄ ν…μ¤νΈ μƒν”μ— λ€ν• μμΈ΅ κ²°κ³Όμ™€ μ •ν™•λ„κ°€ μ¶λ ¥λ¨

Note:
    - ν…μ¤νΈ μƒν”μ€ μμ‹μ©μ΄λ©°, μ‹¤μ  ν‰κ°€ μ‹μ—λ” λ³„λ„μ κ²€μ¦ λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•λ” κ²ƒμ΄ λ°”λμ§ν•¨
"""
# νμΈνλ‹ ν›„ λ¨λΈλ΅ pipeline μƒμ„±
classifier_after = pipeline('text-classification', model=model, tokenizer=tokenizer)

# λ™μΌν• ν…μ¤νΈ μƒν” μ‚¬μ©
test_samples = [
    ("μ‚Όμ„±μ „μ, μ‹ ν• κ°¤λ­μ‹ μ‹λ¦¬μ¦ κ³µκ°", "ITκ³Όν•™"),
    ("μ½”μ¤ν”Ό μ¥μ¤‘ 3000μ„  λν", "κ²½μ "),
    ("μ†ν¥λ―Ό, μ‹μ¦ 15νΈκ³¨ ν­λ°", "μ¤ν¬μΈ "),
    ("μ—¬μ•Ό, μμ‚°μ• μ²λ¦¬ λ‘κ³  μ¶©λ", "μ •μΉ"),
    ("μ „κµ­ λ―Έμ„Έλ¨Όμ§€ 'λ‚μ¨'...μ™Έμ¶ μμ ", "μ‚¬ν"),
]

print("=" * 60)
print("πΆ νμΈνλ‹ ν›„ λ¨λΈ μμΈ΅ κ²°κ³Ό")
print("=" * 60)
print()

correct = 0
for title, true_label in test_samples:
    result = classifier_after(title)[0]
    is_correct = result['label'] == true_label
    correct += is_correct
    status = "β…" if is_correct else "β"

    print(f"μ λ©: {title}")
    print(f"  μ •λ‹µ: {true_label}")
    print(f"  μμΈ΅: {result['label']} (μ‹ λΆ°λ„: {result['score']:.2%}) {status}")
    print()

print(f"μ •ν™•λ„: {correct}/{len(test_samples)} ({correct/len(test_samples)*100:.1f}%)")
print("\nβ¨ νμΈνλ‹ ν›„ μ„±λ¥μ΄ ν¬κ² ν–¥μƒλμ—μµλ‹λ‹¤!")
